{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geolocation Verification and Station Splitting\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import folium\n",
    "from typing import Tuple\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 1965\n",
    "SIDE = \"west\"\n",
    "DATA_DIR = Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 20:49:40,186 - INFO - Loaded 528 stations from OpenRefine\n",
      "2025-02-28 20:49:40,214 - INFO - Loaded 1006 stations from original data\n"
     ]
    }
   ],
   "source": [
    "# Load the OpenRefine processed data\n",
    "refined_data_path = f\"../data/interim/stops_for_openrefine/unmatched_stops_{YEAR}_{SIDE}_refined.csv\"\n",
    "refined_stops = pd.read_csv(refined_data_path)\n",
    "logger.info(f\"Loaded {len(refined_stops)} stations from OpenRefine\")\n",
    "\n",
    "# Load the previously matched stops - this is where the original error was\n",
    "original_stops = pd.read_csv(f'../data/interim/stops_matched_initial/stops_{YEAR}_{SIDE}.csv')\n",
    "logger.info(f\"Loaded {len(original_stops)} stations from original data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Geolocation Format Verification\n",
    "def verify_geo_format(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Verify and standardize geolocation format.\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Function to check and format location string\n",
    "    def format_location(loc_str):\n",
    "        if pd.isna(loc_str) or loc_str == '':\n",
    "            return np.nan\n",
    "            \n",
    "        # Check if it contains multiple locations (with a hyphen)\n",
    "        if ' - ' in loc_str:\n",
    "            # This will be handled separately\n",
    "            return loc_str\n",
    "            \n",
    "        # Remove any extra spaces\n",
    "        loc_str = re.sub(r'\\s+', '', loc_str)\n",
    "        \n",
    "        # Check if it's a valid coordinate pair\n",
    "        pattern = r'^(-?\\d+(\\.\\d+)?),(-?\\d+(\\.\\d+)?)$'\n",
    "        if re.match(pattern, loc_str):\n",
    "            # Valid format, ensure consistent decimal places\n",
    "            lat, lon = map(float, loc_str.split(','))\n",
    "            return f\"{lat:.8f},{lon:.8f}\"\n",
    "        else:\n",
    "            logger.warning(f\"Invalid coordinate format: {loc_str}\")\n",
    "            return np.nan\n",
    "    \n",
    "    # Apply formatting to location column\n",
    "    df['location'] = df['location'].apply(format_location)\n",
    "    \n",
    "    # Count invalid formats\n",
    "    invalid_count = df['location'].isna().sum()\n",
    "    logger.info(f\"Found {invalid_count} stations with invalid coordinate format\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Geographic Bounds Verification\n",
    "def verify_geo_bounds(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Verify coordinates are within expected Berlin bounds.\"\"\"\n",
    "    # Berlin geographic bounds (approximate)\n",
    "    BERLIN_BOUNDS = {\n",
    "        'lat_min': 52.3,\n",
    "        'lat_max': 52.7,\n",
    "        'lon_min': 13.1,\n",
    "        'lon_max': 13.8\n",
    "    }\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    def check_bounds(loc_str):\n",
    "        if pd.isna(loc_str) or loc_str == '':\n",
    "            return False, \"Missing coordinates\"\n",
    "            \n",
    "        # Multiple locations case\n",
    "        if ' - ' in loc_str:\n",
    "            return True, \"Multiple coordinates\"\n",
    "            \n",
    "        try:\n",
    "            lat, lon = map(float, loc_str.split(','))\n",
    "            \n",
    "            if (BERLIN_BOUNDS['lat_min'] <= lat <= BERLIN_BOUNDS['lat_max'] and\n",
    "                BERLIN_BOUNDS['lon_min'] <= lon <= BERLIN_BOUNDS['lon_max']):\n",
    "                return True, \"Within bounds\"\n",
    "            else:\n",
    "                return False, f\"Outside Berlin bounds: {lat},{lon}\"\n",
    "        except:\n",
    "            return False, \"Invalid format\"\n",
    "    \n",
    "    # Check bounds for all locations\n",
    "    results = df['location'].apply(check_bounds)\n",
    "    df['valid_bounds'] = results.apply(lambda x: x[0])\n",
    "    df['bounds_message'] = results.apply(lambda x: x[1])\n",
    "    \n",
    "    # Log locations outside bounds\n",
    "    outside_bounds = df[~df['valid_bounds']]\n",
    "    if not outside_bounds.empty:\n",
    "        logger.warning(f\"Found {len(outside_bounds)} stations outside Berlin bounds:\")\n",
    "        for _, row in outside_bounds.iterrows():\n",
    "            logger.warning(f\"  - {row['stop_name']}: {row['bounds_message']}\")\n",
    "\n",
    "    # Drop the 'bounds_message' and 'valid_bounds' columns\n",
    "    df.drop(columns=['bounds_message', 'valid_bounds'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Handle Stations That Need Splitting\n",
    "def split_combined_stations(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split rows where multiple stations are combined with hyphen.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with stops data        \n",
    "    Returns:\n",
    "        Tuple of (updated_stops_df)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Find rows with combined stations\n",
    "    combined_mask = df['location'].apply(lambda x: isinstance(x, str) and ' - ' in x)\n",
    "    combined_stations = df[combined_mask].copy()\n",
    "    \n",
    "    if combined_stations.empty:\n",
    "        logger.info(\"No combined stations found\")\n",
    "        return df\n",
    "        \n",
    "    logger.info(f\"Found {len(combined_stations)} combined stations to split\")\n",
    "    \n",
    "    # Get the next available stop_id - Make sure all stop_ids are strings for consistency\n",
    "    df['stop_id'] = df['stop_id'].astype(str)\n",
    "    \n",
    "    next_stop_id = int(df['stop_id'].str.replace(r'^\\D*', '', regex=True).astype(int).max()) + 1\n",
    "    \n",
    "    # Process each combined station\n",
    "    for idx, row in combined_stations.iterrows():\n",
    "        # Split station names and locations\n",
    "        stop_names = row['stop_name'].split(' - ')\n",
    "        locations = row['location'].split(' - ')\n",
    "        \n",
    "        if len(stop_names) != len(locations):\n",
    "            logger.warning(f\"Mismatch between names and locations for {row['stop_name']}\")\n",
    "            continue\n",
    "        \n",
    "        # Update the first station in place\n",
    "        df.at[idx, 'stop_name'] = stop_names[0]\n",
    "        df.at[idx, 'location'] = locations[0]\n",
    "        \n",
    "        # Create new rows for additional stations\n",
    "        for i in range(1, len(stop_names)):\n",
    "            new_stop_id = f\"{YEAR}{next_stop_id}\"\n",
    "            next_stop_id += 1\n",
    "            \n",
    "            # Create new row with same attributes but different name/location\n",
    "            new_row = row.copy()\n",
    "            new_row['stop_id'] = new_stop_id\n",
    "            new_row['stop_name'] = stop_names[i]\n",
    "            new_row['location'] = locations[i]\n",
    "            \n",
    "            # Add to dataframe\n",
    "            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            \n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize stations on a map\n",
    "def visualize_stations(df: pd.DataFrame, output_path: str):\n",
    "    \"\"\"Create a folium map with all stations.\"\"\"\n",
    "    # Filter to only valid locations - properly handle empty strings\n",
    "    valid_df = df[(df['location'].notna()) & (df['location'] != '')].copy()\n",
    "    \n",
    "    # Extract coordinates with better error handling\n",
    "    def extract_lat(loc_str):\n",
    "        try:\n",
    "            if not isinstance(loc_str, str) or loc_str == '':\n",
    "                return np.nan\n",
    "            parts = loc_str.split(',')\n",
    "            if len(parts) != 2:\n",
    "                return np.nan\n",
    "            return float(parts[0].strip())\n",
    "        except (ValueError, IndexError):\n",
    "            return np.nan\n",
    "    \n",
    "    def extract_lon(loc_str):\n",
    "        try:\n",
    "            if not isinstance(loc_str, str) or loc_str == '':\n",
    "                return np.nan\n",
    "            parts = loc_str.split(',')\n",
    "            if len(parts) != 2:\n",
    "                return np.nan\n",
    "            return float(parts[1].strip())\n",
    "        except (ValueError, IndexError):\n",
    "            return np.nan\n",
    "    \n",
    "    # Apply coordinate extraction\n",
    "    valid_df['lat'] = valid_df['location'].apply(extract_lat)\n",
    "    valid_df['lon'] = valid_df['location'].apply(extract_lon)\n",
    "    \n",
    "    # Filter out any rows with invalid coordinates\n",
    "    valid_df = valid_df[(valid_df['lat'].notna()) & (valid_df['lon'].notna())]\n",
    "    \n",
    "    logger.info(f\"Creating map with {len(valid_df)} stations\")\n",
    "    \n",
    "    # Create map centered on Berlin\n",
    "    m = folium.Map(location=[52.52, 13.40], zoom_start=12)\n",
    "    \n",
    "    # Define colors for different transport types\n",
    "    type_colors = {\n",
    "        'bus': 'blue',\n",
    "        'strassenbahn': 'red',\n",
    "        'u-bahn': 'green',\n",
    "        's-bahn': 'purple'\n",
    "    }\n",
    "    \n",
    "    # Add markers for each station\n",
    "    for _, row in valid_df.iterrows():\n",
    "        popup_text = f\"{row['stop_name']} ({row['type']})<br>ID: {row['stop_id']}\"\n",
    "        color = type_colors.get(row['type'].lower(), 'gray')\n",
    "        \n",
    "        folium.Marker(\n",
    "            [row['lat'], row['lon']],\n",
    "            popup=popup_text,\n",
    "            icon=folium.Icon(color=color)\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Save map\n",
    "    m.save(output_path)\n",
    "    logger.info(f\"Saved map to {output_path}\")\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 20:49:40,502 - INFO - Found 4 combined stations to split\n",
      "2025-02-28 20:49:40,894 - INFO - Found 0 stations with invalid coordinate format\n",
      "2025-02-28 20:49:40,906 - WARNING - Found 1 stations outside Berlin bounds:\n",
      "2025-02-28 20:49:40,907 - WARNING -   - Glienicker Brücke: Outside Berlin bounds: 52.41347409,13.09123588\n",
      "2025-02-28 20:49:40,918 - INFO - Creating map with 1066 stations\n",
      "2025-02-28 20:49:42,225 - INFO - Saved map to ..\\data\\visualizations\\stations_1965_west.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification complete:\n",
      "Total stations: 1066\n",
      "Valid locations: 1066 (100.0%)\n",
      "Split stations: 60\n"
     ]
    }
   ],
   "source": [
    "# Run all verification steps\n",
    "try:\n",
    "    \n",
    "    # Step 1: Split combined stations\n",
    "    refined_stops = split_combined_stations(refined_stops)\n",
    "\n",
    "    # Merge refined data with original stops based on stop_name, type, and line_name\n",
    "    merged_stops = original_stops.copy()\n",
    "    \n",
    "    for idx, row in refined_stops.iterrows():\n",
    "        stop_name = row['stop_name']\n",
    "        stop_type = row['type']\n",
    "        line_name = row['line_name']\n",
    "        \n",
    "        # Check if this stop exists in the original stops\n",
    "        match = merged_stops[(merged_stops['stop_name'] == stop_name) & \n",
    "                             (merged_stops['type'] == stop_type) & \n",
    "                             (merged_stops['line_name'] == line_name)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            # Update location and identifier if match is found\n",
    "            merged_idx = match.index[0]\n",
    "            merged_stops.at[merged_idx, 'location'] = row['location']\n",
    "            if 'identifier' in row and not pd.isna(row['identifier']):\n",
    "                merged_stops.at[merged_idx, 'identifier'] = row['identifier']\n",
    "        else:\n",
    "            # This is a new stop, add to merged_stops\n",
    "            merged_stops = pd.concat([merged_stops, pd.DataFrame([row])], ignore_index=True)\n",
    "    \n",
    "    # Save updated data\n",
    "    verified_dir = DATA_DIR / 'interim' / 'stops_verified'\n",
    "    verified_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Step 2: Format verification\n",
    "    merged_stops = verify_geo_format(merged_stops)\n",
    "    \n",
    "    # Step 3: Bounds verification\n",
    "    merged_stops = verify_geo_bounds(merged_stops)\n",
    "\n",
    "    # Step 4: Create visualization\n",
    "    map_dir = DATA_DIR / 'visualizations'\n",
    "    map_dir.mkdir(parents=True, exist_ok=True)\n",
    "    visualize_stations(merged_stops, str(map_dir / f'stations_{YEAR}_{SIDE}.html'))\n",
    "    \n",
    "    merged_stops.to_csv(verified_dir / f'stops_{YEAR}_{SIDE}.csv', index=False)\n",
    "    \n",
    "    # Summary statistics\n",
    "    valid_locations = merged_stops['location'].notna().sum()\n",
    "    total_stops = len(merged_stops)\n",
    "    print(f\"\\nVerification complete:\")\n",
    "    print(f\"Total stations: {total_stops}\")\n",
    "    print(f\"Valid locations: {valid_locations} ({valid_locations/total_stops*100:.1f}%)\")\n",
    "    print(f\"Split stations: {len(merged_stops) - len(original_stops)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in verification: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
