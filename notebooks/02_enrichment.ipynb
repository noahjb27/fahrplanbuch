{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import logging\n",
    "import ast\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import re\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "YEAR = 1965\n",
    "SIDE = \"west\"\n",
    "DATA_DIR = Path('../data')\n",
    "RAW_DIR = DATA_DIR / 'raw' / SIDE\n",
    "INTERIM_DIR = DATA_DIR / 'interim'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "GEO_DATA_DIR = DATA_DIR / 'data-external'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Bahn line profile mappings\n",
    "KLEINPROFIL = {'1', '2', '3', '4', 'A', 'A I', 'A II', 'A III', 'A1', 'A2', 'B', 'B I', 'B II', 'B III', 'B1', 'B2'}\n",
    "GROSSPROFIL = {'5', '6', '7', '8', '9', 'C', 'C I', 'C II', 'D', 'E', 'G'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stations\n",
    "matched_path = f\"../data/interim/stops_base/lines_{YEAR}_{SIDE}.csv\"\n",
    "line_df_initial = pd.read_csv(matched_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_lines(line_df):\n",
    "    # Add profile for U-Bahn lines\n",
    "    line_df['profile'] = None\n",
    "    for idx, row in line_df.iterrows():\n",
    "        if row['type'] == 'u-bahn':\n",
    "            if row['line_name'] in KLEINPROFIL:\n",
    "                line_df.at[idx, 'profile'] = 'Kleinprofil'\n",
    "            elif row['line_name'] in GROSSPROFIL:\n",
    "                line_df.at[idx, 'profile'] = 'Großprofil'\n",
    "    \n",
    "    # Add capacity based on transport type and profile\n",
    "    line_df['capacity'] = None\n",
    "    for idx, row in line_df.iterrows():\n",
    "        if row['type'] == 'u-bahn' and row['profile'] == 'Kleinprofil':\n",
    "            line_df.at[idx, 'capacity'] = 750\n",
    "        elif row['type'] == 'u-bahn' and row['profile'] == 'Großprofil':\n",
    "            line_df.at[idx, 'capacity'] = 1000\n",
    "        elif row['type'] == 's-bahn':\n",
    "            line_df.at[idx, 'capacity'] = 1100\n",
    "        elif row['type'] == 'tram':\n",
    "            line_df.at[idx, 'capacity'] = 195\n",
    "        elif row['type'] in ['autobus', 'bus (Umlandlinie)']:\n",
    "            line_df.at[idx, 'capacity'] = 100\n",
    "        elif row['type'] == 'fähre' or row['type'] == 'FÃ¤hre':\n",
    "            line_df.at[idx, 'capacity'] = 300\n",
    "    \n",
    "    return line_df\n",
    "\n",
    "line_df = enrich_lines(line_df_initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 20:56:14,764 - INFO - Enriched stops saved to interim/stops_enriched directory\n"
     ]
    }
   ],
   "source": [
    "# Load stations\n",
    "matched_path = f\"../data/interim/stops_verified/stops_{YEAR}_{SIDE}.csv\"\n",
    "final_stops = pd.read_csv(matched_path)\n",
    "\n",
    "def load_district_data():\n",
    "    \"\"\"Load district boundary data\"\"\"\n",
    "    try:\n",
    "        districts_path = GEO_DATA_DIR / \"lor_ortsteile.geojson\"\n",
    "        districts_gdf = gpd.read_file(districts_path)\n",
    "        \n",
    "        # Load West Berlin districts\n",
    "        west_berlin_path = GEO_DATA_DIR / \"West-Berlin-Ortsteile.json\"\n",
    "        with open(west_berlin_path, \"r\") as f:\n",
    "            west_berlin_districts = json.load(f)[\"West_Berlin\"]\n",
    "            \n",
    "        return districts_gdf, west_berlin_districts\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading district data: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def add_administrative_data(stops_df, districts_gdf, west_berlin_districts):\n",
    "    \"\"\"Add administrative boundary information to stops\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    result_df = stops_df.copy()\n",
    "    \n",
    "    # Filter to only stops with location data\n",
    "    stops_with_location = result_df[result_df['location'].notna()]\n",
    "    \n",
    "    if stops_with_location.empty:\n",
    "        logger.warning(\"No stops with location data found, skipping administrative enrichment\")\n",
    "        return result_df\n",
    "    \n",
    "    try:\n",
    "        # Convert to GeoDataFrame\n",
    "        geometry = stops_with_location['location'].apply(\n",
    "            lambda x: Point(*reversed([float(c.strip()) for c in x.split(',')]))\n",
    "        )\n",
    "        stops_gdf = gpd.GeoDataFrame(stops_with_location, geometry=geometry, crs=\"EPSG:4326\")\n",
    "        \n",
    "        # Ensure CRS matches\n",
    "        if stops_gdf.crs != districts_gdf.crs:\n",
    "            districts_gdf = districts_gdf.to_crs(stops_gdf.crs)\n",
    "        \n",
    "        # Perform spatial join\n",
    "        joined = gpd.sjoin(\n",
    "            stops_gdf, \n",
    "            districts_gdf[['geometry', 'BEZIRK', 'OTEIL']], \n",
    "            how=\"left\", \n",
    "            predicate='within'\n",
    "        )\n",
    "        \n",
    "        # Add east/west classification\n",
    "        joined['east_west_admin'] = joined['OTEIL'].apply(\n",
    "            lambda x: 'west' if pd.notna(x) and x in west_berlin_districts else 'east'\n",
    "        )\n",
    "        \n",
    "        # Drop geometry column and index_right from join\n",
    "        if 'geometry' in joined.columns:\n",
    "            joined = joined.drop(columns=['geometry'])\n",
    "        if 'index_right' in joined.columns:\n",
    "            joined = joined.drop(columns=['index_right'])\n",
    "        \n",
    "        # Update original DataFrame\n",
    "        for idx, row in joined.iterrows():\n",
    "            original_idx = stops_df[stops_df['stop_id'] == row['stop_id']].index[0]\n",
    "            result_df.at[original_idx, 'district'] = row.get('BEZIRK')\n",
    "            result_df.at[original_idx, 'neighborhood'] = row.get('OTEIL')\n",
    "            result_df.at[original_idx, 'east_west_admin'] = row.get('east_west_admin')\n",
    "        \n",
    "        # Validate east/west classification\n",
    "        for idx, row in result_df.iterrows():\n",
    "            if pd.notna(row.get('east_west_admin')) and pd.notna(row.get('east_west')):\n",
    "                if row['east_west_admin'] != row['east_west']:\n",
    "                    logger.warning(f\"Mismatched east/west: {row['stop_name']} - Source says {row['east_west']} but location is in {row['east_west_admin']}\")\n",
    "        \n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error adding administrative data: {str(e)}\")\n",
    "        return result_df\n",
    "\n",
    "# Load district data\n",
    "districts_gdf, west_berlin_districts = load_district_data()\n",
    "\n",
    "# Add administrative data\n",
    "if districts_gdf is not None and west_berlin_districts is not None:\n",
    "    enriched_stops_df = add_administrative_data(final_stops, districts_gdf, west_berlin_districts)\n",
    "    \n",
    "    # Save the enriched stops\n",
    "    (INTERIM_DIR / 'stops_enriched').mkdir(exist_ok=True)\n",
    "    enriched_stops_df.to_csv(INTERIM_DIR / 'stops_enriched' / f'stops_{YEAR}_{SIDE}_enriched.csv', index=False)\n",
    "    \n",
    "    logger.info(f\"Enriched stops saved to interim/stops_enriched directory\")\n",
    "else:\n",
    "    logger.warning(\"Could not load district data, skipping administrative enrichment\")\n",
    "    enriched_stops_df = final_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import geojson\n",
    "\n",
    "\n",
    "# Specify the url for the backend.\n",
    "url = \"https://gdi.berlin.de/services/wfs/postleitzahlen\"\n",
    "\n",
    "# Specify parameters (read data in json format).\n",
    "params = dict(\n",
    "    service=\"WFS\",\n",
    "    version=\"2.0.0\",\n",
    "    request=\"GetFeature\",\n",
    "    typeNames=\"postleitzahlen\",\n",
    "    outputFormat=\"json\",\n",
    ")\n",
    "\n",
    "# Fetch data from WFS using requests\n",
    "r = requests.get(url, params=params)\n",
    "\n",
    "# Create GeoDataFrame from geojson and set coordinate reference system\n",
    "data = gpd.GeoDataFrame.from_features(geojson.loads(r.content), crs=\"EPSG:25833\")\n",
    "\n",
    "# Reproject data_gdf to EPSG:4326\n",
    "data = data.to_crs(epsg=4326)\n",
    "\n",
    "# Convert final_stops to GeoDataFrame\n",
    "geometry = final_stops['location'].apply(lambda x: Point(*reversed([float(c.strip()) for c in x.split(',')])))\n",
    "final_stops_gdf = gpd.GeoDataFrame(final_stops, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# Perform spatial join to find which postal code each stop is in\n",
    "stops_with_plz = gpd.sjoin(final_stops_gdf, data, how='left', predicate='within')\n",
    "# Add the 'plz' column to the original stops_df\n",
    "final_stops['plz'] = stops_with_plz['plz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import raw\n",
    "matched_path = f\"../data/raw/{YEAR}_{SIDE}.csv\"\n",
    "base_df = pd.read_csv(matched_path)\n",
    "\n",
    "def create_line_stops_df(df):\n",
    "    line_stops = df['stops'].str.split(' - ', expand=True).stack().reset_index(level=1, drop=True).reset_index(name='stop_name')\n",
    "\n",
    "    line_stops['stop_order'] = line_stops.groupby('index').cumcount()\n",
    "    #index starts from 0 so it looks like 1 row is missing but this is not true\n",
    "\n",
    "    # Clean the 'Stop Name' column by removing whitespace and non-breaking spaces\n",
    "    line_stops['stop_name'] = line_stops['stop_name'].str.replace(u'\\xa0', ' ').str.strip()\n",
    "\n",
    "    # reset index so that it can be used for foreign key\n",
    "\n",
    "    return line_stops\n",
    "\n",
    "line_stops = create_line_stops_df(base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increment the index by 1 and set it as 'line_id'\n",
    "line_stops['line_id'] = str(YEAR) + (line_stops[\"index\"] + 1).astype(str)\n",
    "# Drop the 'index' column\n",
    "line_stops.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_type(line_stops, line_df):\n",
    "    # Ensure line_id columns are of the same type\n",
    "    line_stops['line_id'] = line_stops['line_id'].astype(str)\n",
    "    line_df['line_id'] = line_df['line_id'].astype(str)\n",
    "    \n",
    "    # Assuming line_id is the common column between line_stops and line_df\n",
    "    merged_df = pd.merge(line_stops, line_df[['line_id', 'type', \"line_name\"]], on='line_id', how='left')\n",
    "    \n",
    "    # Rename the 'type' column from line_df to 'type_from_line_df' to avoid conflicts\n",
    "    merged_df.rename(columns={'type': 'type'}, inplace=True)\n",
    "    \n",
    "    # Drop the 'type_from_line_df' column if it's not needed in the final result\n",
    "    # merged_df.drop(columns=['type_from_line_df'], inplace=True)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "line_stops = add_type(line_stops, line_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fk(line_stops, df_stops):\n",
    "    # Create a new dataframe with the Stop Name and Stop ID columns\n",
    "    stop_id_df = df_stops[['stop_name', 'stop_id', 'type']]\n",
    "\n",
    "    # Merge the line_stops and stop_id_df dataframes based on matching stop names and type\n",
    "    line_stops = line_stops.merge(stop_id_df,\n",
    "                                  left_on=['stop_name', 'type'],\n",
    "                                  right_on=['stop_name', 'type'],\n",
    "                                  how='left')\n",
    "\n",
    "    return line_stops\n",
    "\n",
    "# Assuming line_stops and df_stops are your dataframes\n",
    "# Replace 'stop_name', 'stop_id', 'type', and 'line_name' with the actual column names you have\n",
    "\n",
    "line_stops = add_fk(line_stops, final_stops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_name</th>\n",
       "      <th>stop_order</th>\n",
       "      <th>line_id</th>\n",
       "      <th>type</th>\n",
       "      <th>line_name</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [stop_name, stop_order, line_id, type, line_name, stop_id, diff]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the difference between consecutive 'stop_order' values\n",
    "line_stops['diff'] = line_stops['stop_order'].diff()\n",
    "\n",
    "# Use the following line to drop rows where the 'diff' column is 0.0\n",
    "line_stops = line_stops[line_stops['diff'] != 0.0]\n",
    "\n",
    "# Identify faulty rows where the difference is not 1 digit behind\n",
    "faulty_rows = line_stops[(line_stops['diff'] != 1) & (line_stops['stop_order'] != 0)]\n",
    "faulty_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_name</th>\n",
       "      <th>stop_order</th>\n",
       "      <th>line_id</th>\n",
       "      <th>type</th>\n",
       "      <th>line_name</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marienfelde, Daimlerstrasse</td>\n",
       "      <td>0</td>\n",
       "      <td>19651</td>\n",
       "      <td>tram</td>\n",
       "      <td>15</td>\n",
       "      <td>19650</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Großbeerenstrasse Ecke Daimlerstrasse</td>\n",
       "      <td>1</td>\n",
       "      <td>19651</td>\n",
       "      <td>tram</td>\n",
       "      <td>15</td>\n",
       "      <td>19651</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Körtingstrasse Ecke Großbeerenstrasse</td>\n",
       "      <td>2</td>\n",
       "      <td>19651</td>\n",
       "      <td>tram</td>\n",
       "      <td>15</td>\n",
       "      <td>19652</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mariendorferdamm Ecke Alt-Mariendorf</td>\n",
       "      <td>3</td>\n",
       "      <td>19651</td>\n",
       "      <td>tram</td>\n",
       "      <td>15</td>\n",
       "      <td>19653</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Imbrosweg Ecke Rixdorferstrasse</td>\n",
       "      <td>4</td>\n",
       "      <td>19651</td>\n",
       "      <td>tram</td>\n",
       "      <td>15</td>\n",
       "      <td>19654</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>Turmstrasse</td>\n",
       "      <td>4</td>\n",
       "      <td>1965102</td>\n",
       "      <td>u-bahn</td>\n",
       "      <td>G</td>\n",
       "      <td>19651004</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>Hansaplatz</td>\n",
       "      <td>5</td>\n",
       "      <td>1965102</td>\n",
       "      <td>u-bahn</td>\n",
       "      <td>G</td>\n",
       "      <td>19651005</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>Zoologischer Garten</td>\n",
       "      <td>6</td>\n",
       "      <td>1965102</td>\n",
       "      <td>u-bahn</td>\n",
       "      <td>G</td>\n",
       "      <td>1965941</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>Kurfürstendamm</td>\n",
       "      <td>7</td>\n",
       "      <td>1965102</td>\n",
       "      <td>u-bahn</td>\n",
       "      <td>G</td>\n",
       "      <td>1965965</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>Spichernstrasse</td>\n",
       "      <td>8</td>\n",
       "      <td>1965102</td>\n",
       "      <td>u-bahn</td>\n",
       "      <td>G</td>\n",
       "      <td>1965950</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1396 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  stop_name  stop_order  line_id    type  \\\n",
       "0               Marienfelde, Daimlerstrasse           0    19651    tram   \n",
       "1     Großbeerenstrasse Ecke Daimlerstrasse           1    19651    tram   \n",
       "2     Körtingstrasse Ecke Großbeerenstrasse           2    19651    tram   \n",
       "3      Mariendorferdamm Ecke Alt-Mariendorf           3    19651    tram   \n",
       "4           Imbrosweg Ecke Rixdorferstrasse           4    19651    tram   \n",
       "...                                     ...         ...      ...     ...   \n",
       "1434                            Turmstrasse           4  1965102  u-bahn   \n",
       "1435                             Hansaplatz           5  1965102  u-bahn   \n",
       "1436                    Zoologischer Garten           6  1965102  u-bahn   \n",
       "1437                         Kurfürstendamm           7  1965102  u-bahn   \n",
       "1438                        Spichernstrasse           8  1965102  u-bahn   \n",
       "\n",
       "     line_name   stop_id  diff  \n",
       "0           15     19650   NaN  \n",
       "1           15     19651   1.0  \n",
       "2           15     19652   1.0  \n",
       "3           15     19653   1.0  \n",
       "4           15     19654   1.0  \n",
       "...        ...       ...   ...  \n",
       "1434         G  19651004   1.0  \n",
       "1435         G  19651005   1.0  \n",
       "1436         G   1965941   1.0  \n",
       "1437         G   1965965   1.0  \n",
       "1438         G   1965950   1.0  \n",
       "\n",
       "[1396 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values in 'stop_id' column are numeric\n"
     ]
    }
   ],
   "source": [
    "# convert 'Stop ID' column to numeric values, coercing errors to NaN\n",
    "line_stops['stop_id'] = pd.to_numeric(line_stops['stop_id'], errors='coerce')\n",
    "\n",
    "# check if all values in 'Stop ID' column are numeric\n",
    "if line_stops['stop_id'].notnull().all():\n",
    "    print(\"All values in 'stop_id' column are numeric\")\n",
    "else:\n",
    "    print(\"There are non-numeric values in 'stop_id' column\")\n",
    "    print(line_stops[line_stops['stop_id'].isnull()])\n",
    "\n",
    "line_stops.drop(['line_name', \"stop_name\", \"type\", \"diff\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 20:57:07,972 - INFO - Final data saved to processed directory\n"
     ]
    }
   ],
   "source": [
    "def finalize_data(line_df, stops_df, line_stops):\n",
    "    \"\"\"Prepare final datasets for output\"\"\"\n",
    "    # Make copies to avoid modifying originals\n",
    "    final_line_df = line_df.copy()\n",
    "    final_stops_df = stops_df.copy()\n",
    "    final_line_stops_df = line_stops.copy()\n",
    "    \n",
    "    # Ensure consistent column names and formats\n",
    "    \n",
    "    # For lines table\n",
    "    if 'profile' not in final_line_df.columns:\n",
    "        final_line_df['profile'] = None\n",
    "    \n",
    "    # Sort tables\n",
    "    final_line_df = final_line_df.sort_values('line_id')\n",
    "    final_stops_df = final_stops_df.sort_values('stop_id')\n",
    "    final_line_stops_df = final_line_stops_df.sort_values(['line_id', 'stop_order'])\n",
    "    \n",
    "    return final_line_df, final_stops_df, final_line_stops_df\n",
    "\n",
    "# Finalize data\n",
    "final_line_df, final_stops_df, final_line_stops_df = finalize_data(line_df, enriched_stops_df, line_stops)\n",
    "\n",
    "# Save final data\n",
    "(PROCESSED_DIR / str(YEAR)).mkdir(parents=True, exist_ok=True)\n",
    "final_line_df.to_csv(PROCESSED_DIR / str(YEAR) / f'lines_{YEAR}_{SIDE}.csv', index=False)\n",
    "final_stops_df.to_csv(PROCESSED_DIR / str(YEAR) / f'stops_{YEAR}_{SIDE}.csv', index=False)\n",
    "final_line_stops_df.to_csv(PROCESSED_DIR / str(YEAR) / f'line_stops_{YEAR}_{SIDE}.csv', index=False)\n",
    "\n",
    "logger.info(f\"Final data saved to processed directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 20:57:08,028 - INFO - STAGE 8: Updating reference data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 20:57:08,526 - INFO - Added 0 new stops to reference data\n",
      "2025-02-28 20:57:08,527 - INFO - Updated locations for 0 existing stops\n",
      "2025-02-28 20:57:08,541 - INFO - Updated reference data saved to processed directory\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# STAGE 8: UPDATE REFERENCE DATA\n",
    "# =============================================\n",
    "\n",
    "# load existing stations\n",
    "existing_stations_df = pd.read_csv(\"../data/processed/existing_stations.csv\")\n",
    "\n",
    "logger.info(\"STAGE 8: Updating reference data\")\n",
    "\n",
    "def update_reference_data(new_stops_df, existing_stations_df):\n",
    "    \"\"\"Update the reference stations dataset with new geolocated stops\"\"\"\n",
    "    # Make a copy of existing stations\n",
    "    updated_stations = existing_stations_df.copy() if existing_stations_df is not None else pd.DataFrame()\n",
    "    \n",
    "    # Filter to stops with location\n",
    "    new_stops_with_location = new_stops_df[new_stops_df['location'].notna()]\n",
    "    \n",
    "    # Create a set of existing stop identifiers (stop_name + type)\n",
    "    if not updated_stations.empty:\n",
    "        existing_identifiers = set(\n",
    "            updated_stations.apply(lambda x: f\"{x['stop_name']}|{x['type']}\", axis=1)\n",
    "        )\n",
    "    else:\n",
    "        existing_identifiers = set()\n",
    "    \n",
    "    # Add new stops that aren't in the existing stations\n",
    "    new_stops = []\n",
    "    updated_count = 0\n",
    "    \n",
    "    for _, stop in new_stops_with_location.iterrows():\n",
    "        stop_identifier = f\"{stop['stop_name']}|{stop['type']}\"\n",
    "        \n",
    "        if stop_identifier not in existing_identifiers:\n",
    "            # This is a new stop, add it\n",
    "            new_stops.append(stop)\n",
    "        else:\n",
    "            # This is an existing stop, update location if it was missing\n",
    "            matching_idx = updated_stations[\n",
    "                (updated_stations['stop_name'] == stop['stop_name']) &\n",
    "                (updated_stations['type'] == stop['type'])\n",
    "            ].index\n",
    "            \n",
    "            if not matching_idx.empty:\n",
    "                idx = matching_idx[0]\n",
    "                if pd.isna(updated_stations.at[idx, 'location']):\n",
    "                    updated_stations.at[idx, 'location'] = stop['location']\n",
    "                    updated_count += 1\n",
    "    \n",
    "    # Convert new stops to DataFrame\n",
    "    if new_stops:\n",
    "        new_stops_df = pd.DataFrame(new_stops)\n",
    "        \n",
    "        # Ensure consistent columns with existing_stations\n",
    "        if not updated_stations.empty:\n",
    "            for col in updated_stations.columns:\n",
    "                if col not in new_stops_df.columns:\n",
    "                    new_stops_df[col] = None\n",
    "            \n",
    "            # Select only the columns in existing_stations\n",
    "            new_stops_df = new_stops_df[updated_stations.columns]\n",
    "        \n",
    "        # Combine with existing stations\n",
    "        updated_stations = pd.concat([updated_stations, new_stops_df], ignore_index=True)\n",
    "    \n",
    "    logger.info(f\"Added {len(new_stops)} new stops to reference data\")\n",
    "    logger.info(f\"Updated locations for {updated_count} existing stops\")\n",
    "    \n",
    "    return updated_stations\n",
    "\n",
    "# Update reference data\n",
    "updated_reference = update_reference_data(final_stops_df, existing_stations_df)\n",
    "\n",
    "# Save updated reference data\n",
    "updated_reference.to_csv(PROCESSED_DIR / 'existing_stations.csv', index=False)\n",
    "\n",
    "logger.info(f\"Updated reference data saved to processed directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 20:57:08,562 - INFO - Processing complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SUMMARY: 1965 WEST BERLIN PROCESSING\n",
      "==================================================\n",
      "Lines processed: 102\n",
      "Stops processed: 1066\n",
      "Line-stop relationships processed: 1396\n",
      "Stops with location data: 1066 (100.0%)\n",
      "Stops with district data: 1010 (94.7%)\n",
      "\n",
      "Files created:\n",
      "- ..\\data\\processed\\lines_1965_west.csv\n",
      "- ..\\data\\processed\\stops_1965_west.csv\n",
      "- ..\\data\\processed\\line_stops_1965_west.csv\n",
      "- ..\\data\\processed\\existing_stations.csv (updated reference data)\n",
      "\n",
      "Next steps:\n",
      "1. Review and manually correct any stops missing location data\n",
      "2. Process East Berlin 1965 data using the same workflow\n",
      "3. Compare West and East Berlin networks using visualization tools\n",
      "4. Proceed to temporal analysis by processing adjacent years (1964, 1966)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Processing complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"SUMMARY: {YEAR} {SIDE.upper()} BERLIN PROCESSING\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Lines processed: {len(final_line_df)}\")\n",
    "print(f\"Stops processed: {len(final_stops_df)}\")\n",
    "print(f\"Line-stop relationships processed: {len(final_line_stops_df)}\")\n",
    "print(f\"Stops with location data: {final_stops_df['location'].notna().sum()} ({final_stops_df['location'].notna().sum()/len(final_stops_df)*100:.1f}%)\")\n",
    "\n",
    "if 'district' in final_stops_df.columns:\n",
    "    print(f\"Stops with district data: {final_stops_df['district'].notna().sum()} ({final_stops_df['district'].notna().sum()/len(final_stops_df)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nFiles created:\")\n",
    "print(f\"- {PROCESSED_DIR / f'lines_{YEAR}_{SIDE}.csv'}\")\n",
    "print(f\"- {PROCESSED_DIR / f'stops_{YEAR}_{SIDE}.csv'}\")\n",
    "print(f\"- {PROCESSED_DIR / f'line_stops_{YEAR}_{SIDE}.csv'}\")\n",
    "print(f\"- {PROCESSED_DIR / 'existing_stations.csv'} (updated reference data)\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review and manually correct any stops missing location data\")\n",
    "print(\"2. Process East Berlin 1965 data using the same workflow\")\n",
    "print(\"3. Compare West and East Berlin networks using visualization tools\")\n",
    "print(\"4. Proceed to temporal analysis by processing adjacent years (1964, 1966)\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
