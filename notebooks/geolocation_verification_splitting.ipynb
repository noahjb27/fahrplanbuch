{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geolocation Verification and Station Splitting\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 1965\n",
    "SIDE = \"west\"\n",
    "DATA_DIR = Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 01:07:30,243 - INFO - Loaded 528 stations from OpenRefine\n",
      "2025-02-28 01:07:30,243 - INFO - Loaded 1006 stations from original data\n",
      "2025-02-28 01:07:30,243 - INFO - Loaded 204 line-stop relationships\n"
     ]
    }
   ],
   "source": [
    "# Load the OpenRefine processed data\n",
    "refined_data_path = f\"../data/interim/stops_for_openrefine/unmatched_stops_{YEAR}_{SIDE}_refined.csv\"\n",
    "refined_stops = pd.read_csv(refined_data_path)\n",
    "logger.info(f\"Loaded {len(refined_stops)} stations from OpenRefine\")\n",
    "\n",
    "# Load the previously matched stops - this is where the original error was\n",
    "original_stops = pd.read_csv(f'../data/interim/stops_matched_initial/stops_{YEAR}_{SIDE}.csv')\n",
    "logger.info(f\"Loaded {len(original_stops)} stations from original data\")\n",
    "\n",
    "# Also load line_stops to update later\n",
    "line_stops_path = f\"../data/interim/stops_base/line_stops_{YEAR}_{SIDE}.csv\"\n",
    "line_stops = pd.read_csv(line_stops_path)\n",
    "logger.info(f\"Loaded {len(line_stops)} line-stop relationships\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Geolocation Format Verification\n",
    "def verify_geo_format(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Verify and standardize geolocation format.\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Function to check and format location string\n",
    "    def format_location(loc_str):\n",
    "        if pd.isna(loc_str) or loc_str == '':\n",
    "            return np.nan\n",
    "            \n",
    "        # Check if it contains multiple locations (with a hyphen)\n",
    "        if ' - ' in loc_str:\n",
    "            # This will be handled separately\n",
    "            return loc_str\n",
    "            \n",
    "        # Remove any extra spaces\n",
    "        loc_str = re.sub(r'\\s+', '', loc_str)\n",
    "        \n",
    "        # Check if it's a valid coordinate pair\n",
    "        pattern = r'^(-?\\d+(\\.\\d+)?),(-?\\d+(\\.\\d+)?)$'\n",
    "        if re.match(pattern, loc_str):\n",
    "            # Valid format, ensure consistent decimal places\n",
    "            lat, lon = map(float, loc_str.split(','))\n",
    "            return f\"{lat:.8f},{lon:.8f}\"\n",
    "        else:\n",
    "            logger.warning(f\"Invalid coordinate format: {loc_str}\")\n",
    "            return np.nan\n",
    "    \n",
    "    # Apply formatting to location column\n",
    "    df['location'] = df['location'].apply(format_location)\n",
    "    \n",
    "    # Count invalid formats\n",
    "    invalid_count = df['location'].isna().sum()\n",
    "    logger.info(f\"Found {invalid_count} stations with invalid coordinate format\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Geographic Bounds Verification\n",
    "def verify_geo_bounds(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Verify coordinates are within expected Berlin bounds.\"\"\"\n",
    "    # Berlin geographic bounds (approximate)\n",
    "    BERLIN_BOUNDS = {\n",
    "        'lat_min': 52.3,\n",
    "        'lat_max': 52.7,\n",
    "        'lon_min': 13.1,\n",
    "        'lon_max': 13.8\n",
    "    }\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    def check_bounds(loc_str):\n",
    "        if pd.isna(loc_str) or loc_str == '':\n",
    "            return False, \"Missing coordinates\"\n",
    "            \n",
    "        # Multiple locations case\n",
    "        if ' - ' in loc_str:\n",
    "            return True, \"Multiple coordinates\"\n",
    "            \n",
    "        try:\n",
    "            lat, lon = map(float, loc_str.split(','))\n",
    "            \n",
    "            if (BERLIN_BOUNDS['lat_min'] <= lat <= BERLIN_BOUNDS['lat_max'] and\n",
    "                BERLIN_BOUNDS['lon_min'] <= lon <= BERLIN_BOUNDS['lon_max']):\n",
    "                return True, \"Within bounds\"\n",
    "            else:\n",
    "                return False, f\"Outside Berlin bounds: {lat},{lon}\"\n",
    "        except:\n",
    "            return False, \"Invalid format\"\n",
    "    \n",
    "    # Check bounds for all locations\n",
    "    results = df['location'].apply(check_bounds)\n",
    "    df['valid_bounds'] = results.apply(lambda x: x[0])\n",
    "    df['bounds_message'] = results.apply(lambda x: x[1])\n",
    "    \n",
    "    # Log locations outside bounds\n",
    "    outside_bounds = df[~df['valid_bounds']]\n",
    "    if not outside_bounds.empty:\n",
    "        logger.warning(f\"Found {len(outside_bounds)} stations outside Berlin bounds:\")\n",
    "        for _, row in outside_bounds.iterrows():\n",
    "            logger.warning(f\"  - {row['stop_name']}: {row['bounds_message']}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Handle Stations That Need Splitting\n",
    "def split_combined_stations(df: pd.DataFrame, line_stops_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split rows where multiple stations are combined with hyphen.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with stops data\n",
    "        line_stops_df: DataFrame with line-stop relationships\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (updated_stops_df, updated_line_stops_df)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    line_stops_df = line_stops_df.copy()\n",
    "    \n",
    "    # Find rows with combined stations\n",
    "    combined_mask = df['location'].apply(lambda x: isinstance(x, str) and ' - ' in x)\n",
    "    combined_stations = df[combined_mask].copy()\n",
    "    \n",
    "    if combined_stations.empty:\n",
    "        logger.info(\"No combined stations found\")\n",
    "        return df, line_stops_df\n",
    "        \n",
    "    logger.info(f\"Found {len(combined_stations)} combined stations to split\")\n",
    "    \n",
    "    # Get the next available stop_id - Make sure all stop_ids are strings for consistency\n",
    "    line_stops_df['stop_id'] = line_stops_df['stop_id'].astype(str)\n",
    "    df['stop_id'] = df['stop_id'].astype(str)\n",
    "    \n",
    "    next_stop_id = int(df['stop_id'].str.replace(r'^\\D*', '', regex=True).astype(int).max()) + 1\n",
    "    \n",
    "    # Process each combined station\n",
    "    for idx, row in combined_stations.iterrows():\n",
    "        # Split station names and locations\n",
    "        stop_names = row['stop_name'].split(' - ')\n",
    "        locations = row['location'].split(' - ')\n",
    "        \n",
    "        if len(stop_names) != len(locations):\n",
    "            logger.warning(f\"Mismatch between names and locations for {row['stop_name']}\")\n",
    "            continue\n",
    "            \n",
    "        # Create new entries for each split station\n",
    "        original_stop_id = row['stop_id']\n",
    "        \n",
    "        # Update the first station in place\n",
    "        df.at[idx, 'stop_name'] = stop_names[0]\n",
    "        df.at[idx, 'location'] = locations[0]\n",
    "        \n",
    "        # Create new rows for additional stations\n",
    "        for i in range(1, len(stop_names)):\n",
    "            new_stop_id = f\"{YEAR}{next_stop_id}\"\n",
    "            next_stop_id += 1\n",
    "            \n",
    "            # Create new row with same attributes but different name/location\n",
    "            new_row = row.copy()\n",
    "            new_row['stop_id'] = new_stop_id\n",
    "            new_row['stop_name'] = stop_names[i]\n",
    "            new_row['location'] = locations[i]\n",
    "            \n",
    "            # Add to dataframe\n",
    "            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            \n",
    "            # Update line_stops references\n",
    "            # Find all references to the original stop_id\n",
    "            line_refs = line_stops_df[line_stops_df['stop_id'] == original_stop_id]\n",
    "            \n",
    "            # Create additional line_stops entries for the new stop_id\n",
    "            for _, line_ref in line_refs.iterrows():\n",
    "                new_line_ref = line_ref.copy()\n",
    "                new_line_ref['stop_id'] = new_stop_id  # This is now a string\n",
    "                line_stops_df = pd.concat([line_stops_df, pd.DataFrame([new_line_ref])], ignore_index=True)\n",
    "                \n",
    "    return df, line_stops_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize stations on a map\n",
    "def visualize_stations(df: pd.DataFrame, output_path: str):\n",
    "    \"\"\"Create a folium map with all stations.\"\"\"\n",
    "    # Filter to only valid locations - properly handle empty strings\n",
    "    valid_df = df[(df['location'].notna()) & (df['location'] != '')].copy()\n",
    "    \n",
    "    # Extract coordinates with better error handling\n",
    "    def extract_lat(loc_str):\n",
    "        try:\n",
    "            if not isinstance(loc_str, str) or loc_str == '':\n",
    "                return np.nan\n",
    "            parts = loc_str.split(',')\n",
    "            if len(parts) != 2:\n",
    "                return np.nan\n",
    "            return float(parts[0].strip())\n",
    "        except (ValueError, IndexError):\n",
    "            return np.nan\n",
    "    \n",
    "    def extract_lon(loc_str):\n",
    "        try:\n",
    "            if not isinstance(loc_str, str) or loc_str == '':\n",
    "                return np.nan\n",
    "            parts = loc_str.split(',')\n",
    "            if len(parts) != 2:\n",
    "                return np.nan\n",
    "            return float(parts[1].strip())\n",
    "        except (ValueError, IndexError):\n",
    "            return np.nan\n",
    "    \n",
    "    # Apply coordinate extraction\n",
    "    valid_df['lat'] = valid_df['location'].apply(extract_lat)\n",
    "    valid_df['lon'] = valid_df['location'].apply(extract_lon)\n",
    "    \n",
    "    # Filter out any rows with invalid coordinates\n",
    "    valid_df = valid_df[(valid_df['lat'].notna()) & (valid_df['lon'].notna())]\n",
    "    \n",
    "    logger.info(f\"Creating map with {len(valid_df)} stations\")\n",
    "    \n",
    "    # Create map centered on Berlin\n",
    "    m = folium.Map(location=[52.52, 13.40], zoom_start=12)\n",
    "    \n",
    "    # Define colors for different transport types\n",
    "    type_colors = {\n",
    "        'bus': 'blue',\n",
    "        'strassenbahn': 'red',\n",
    "        'u-bahn': 'green',\n",
    "        's-bahn': 'purple'\n",
    "    }\n",
    "    \n",
    "    # Add markers for each station\n",
    "    for _, row in valid_df.iterrows():\n",
    "        popup_text = f\"{row['stop_name']} ({row['type']})<br>ID: {row['stop_id']}\"\n",
    "        color = type_colors.get(row['type'].lower(), 'gray')\n",
    "        \n",
    "        folium.Marker(\n",
    "            [row['lat'], row['lon']],\n",
    "            popup=popup_text,\n",
    "            icon=folium.Icon(color=color)\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Save map\n",
    "    m.save(output_path)\n",
    "    logger.info(f\"Saved map to {output_path}\")\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 01:07:30,289 - INFO - Found 4 combined stations to split\n",
      "2025-02-28 01:07:30,573 - INFO - Found 0 stations with invalid coordinate format\n",
      "2025-02-28 01:07:30,573 - WARNING - Found 1 stations outside Berlin bounds:\n",
      "2025-02-28 01:07:30,577 - WARNING -   - Glienicker Brücke: Outside Berlin bounds: 52.41347409,13.09123588\n",
      "2025-02-28 01:07:30,577 - INFO - Creating map with 1024 stations\n",
      "2025-02-28 01:07:31,369 - INFO - Saved map to ..\\data\\visualizations\\stations_1965_west.html\n",
      "2025-02-28 01:07:31,375 - INFO - Saved verified stops and line_stops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification complete:\n",
      "Total stations: 1024\n",
      "Valid locations: 1024 (100.0%)\n",
      "Split stations: 18\n"
     ]
    }
   ],
   "source": [
    "# Run all verification steps\n",
    "try:\n",
    "    \n",
    "    # Step 1: Split combined stations\n",
    "    refined_stops, line_stops = split_combined_stations(refined_stops, line_stops)\n",
    "\n",
    "    # Merge refined data with original stops based on stop_name, type, and line_name\n",
    "    merged_stops = original_stops.copy()\n",
    "    \n",
    "    for idx, row in refined_stops.iterrows():\n",
    "        stop_name = row['stop_name']\n",
    "        stop_type = row['type']\n",
    "        line_name = row['line_name']\n",
    "        \n",
    "        # Check if this stop exists in the original stops\n",
    "        match = merged_stops[(merged_stops['stop_name'] == stop_name) & \n",
    "                             (merged_stops['type'] == stop_type) & \n",
    "                             (merged_stops['line_name'] == line_name)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            # Update location and identifier if match is found\n",
    "            merged_idx = match.index[0]\n",
    "            merged_stops.at[merged_idx, 'location'] = row['location']\n",
    "            if 'identifier' in row and not pd.isna(row['identifier']):\n",
    "                merged_stops.at[merged_idx, 'identifier'] = row['identifier']\n",
    "        else:\n",
    "            # This is a new stop, add to merged_stops\n",
    "            merged_stops = pd.concat([merged_stops, pd.DataFrame([row])], ignore_index=True)\n",
    "    \n",
    "    # Save updated data\n",
    "    verified_dir = DATA_DIR / 'interim' / 'stops_verified'\n",
    "    verified_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Step 2: Format verification\n",
    "    merged_stops = verify_geo_format(merged_stops)\n",
    "    \n",
    "    # Step 3: Bounds verification\n",
    "    merged_stops = verify_geo_bounds(merged_stops)\n",
    "\n",
    "    # Step 4: Create visualization\n",
    "    map_dir = DATA_DIR / 'visualizations'\n",
    "    map_dir.mkdir(parents=True, exist_ok=True)\n",
    "    visualize_stations(merged_stops, str(map_dir / f'stations_{YEAR}_{SIDE}.html'))\n",
    "    \n",
    "    merged_stops.to_csv(verified_dir / f'stops_{YEAR}_{SIDE}.csv', index=False)\n",
    "    line_stops.to_csv(verified_dir / f'line_stops_{YEAR}_{SIDE}.csv', index=False)\n",
    "    logger.info(f\"Saved verified stops and line_stops\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    valid_locations = merged_stops['location'].notna().sum()\n",
    "    total_stops = len(merged_stops)\n",
    "    print(f\"\\nVerification complete:\")\n",
    "    print(f\"Total stations: {total_stops}\")\n",
    "    print(f\"Valid locations: {valid_locations} ({valid_locations/total_stops*100:.1f}%)\")\n",
    "    print(f\"Split stations: {len(merged_stops) - len(original_stops)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in verification: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
